{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "## 1、ChatMessageHistory(基础)的使用",
   "id": "8354ea905e90ce72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T06:40:15.163990Z",
     "start_time": "2025-10-04T06:40:12.079420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains.summarize.map_reduce_prompt import prompt_template\n",
    "# 场景一：记忆存储\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 1、ChatMessageHistory实例化\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "# 2、添加消息\n",
    "history.add_user_message(\"你好\")\n",
    "history.add_ai_message(\"你好！有什么我可以帮助你的吗？\")\n",
    "\n",
    "# 3、获取消息\n",
    "messages = history.messages\n",
    "for message in messages:\n",
    "\tprint(f\"{message.type}: {message.content}\")"
   ],
   "id": "11fd3506af70a24a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: 你好\n",
      "ai: 你好！有什么我可以帮助你的吗？\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T06:45:26.415092Z",
     "start_time": "2025-10-04T06:45:24.418004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 场景二：与LLM集成\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ChatMessageHistory\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "# 1、加载环境变量\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY1\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "# 2、实例化ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "# 3、实例化ChatMessageHistory\n",
    "history = ChatMessageHistory()\n",
    "# 4、添加用户消息\n",
    "history.add_user_message(\"你好\")\n",
    "# 5、获取消息并调用LLM\n",
    "response = llm.invoke(history.messages)\n",
    "print(\"AI:\", response.content)\n",
    "# 6、添加AI消息到历史\n",
    "history.add_ai_message(response.content)\n",
    "# 7、继续对话\n",
    "history.add_user_message(\"今天天气怎么样？\")\n",
    "response = llm.invoke(history.messages)\n",
    "print(\"AI:\", response.content)\n",
    "history.add_ai_message(response.content)\n",
    "# 8、查看完整对话历史\n",
    "for message in history.messages:\n",
    "\tprint(f\"{message.type}: {message.content}\")"
   ],
   "id": "4cc83213d52990a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: 你好！有什么我可以帮助你的吗？\n",
      "AI: 我无法实时获取天气信息，但你可以通过天气应用、网站或当地新闻来查看今天天气。如果你告诉我你所在的城市，我可以给你一些建议，如何查找天气信息！\n",
      "human: 你好\n",
      "ai: 你好！有什么我可以帮助你的吗？\n",
      "human: 今天天气怎么样？\n",
      "ai: 我无法实时获取天气信息，但你可以通过天气应用、网站或当地新闻来查看今天天气。如果你告诉我你所在的城市，我可以给你一些建议，如何查找天气信息！\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2、ConversationBufferMemory的使用\n",
    "适用场景：对话轮次较少、依赖完整上下文的场景（如简单的聊天机器）"
   ],
   "id": "1ca84558ddbe0af7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T14:42:20.422828Z",
     "start_time": "2025-10-04T14:42:20.411854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 举例1：以字符串的方式返回存储的信息\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 1、ConversationBufferMemory()的实例化\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# 2、存储相关的信息\n",
    "# 注意：inputs对应的是用户输入，outputs对应的是AI的回复\n",
    "memory.save_context(\n",
    "\tinputs={\n",
    "\t\t\"input\": \"你好\"\n",
    "\t},\n",
    "\toutputs={\n",
    "\t\t\"output\": \"你好！有什么我可以帮助你的吗？\"\n",
    "\t}\n",
    ")\n",
    "memory.save_context(\n",
    "\tinputs={\n",
    "\t\t\"input\": \"请问1.8何1.11谁大？\"\n",
    "\t},\n",
    "\toutputs={\n",
    "\t\t\"output\": \"1.8比较大\"\n",
    "\t}\n",
    ")\n",
    "\n",
    "# 3、获取存储的信息\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "# 返回的字典结构的key叫做history，value是一个字符串"
   ],
   "id": "d4c99d25298978a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 你好\\nAI: 你好！有什么我可以帮助你的吗？\\nHuman: 请问1.8何1.11谁大？\\nAI: 1.8比较大'}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T14:47:45.869499Z",
     "start_time": "2025-10-04T14:47:45.857532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 举例2：以消息列表的方式返回存储的信息\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 1、ConversationBufferMemory()的实例化\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# 2、存储相关的信息\n",
    "# 注意：inputs对应的是用户输入，outputs对应的是AI的回复\n",
    "memory.save_context(\n",
    "\tinputs={\n",
    "\t\t\"input\": \"你好\"\n",
    "\t},\n",
    "\toutputs={\n",
    "\t\t\"output\": \"你好！有什么我可以帮助你的吗？\"\n",
    "\t}\n",
    ")\n",
    "memory.save_context(\n",
    "\tinputs={\n",
    "\t\t\"input\": \"请问1.8何1.11谁大？\"\n",
    "\t},\n",
    "\toutputs={\n",
    "\t\t\"output\": \"1.8比较大\"\n",
    "\t}\n",
    ")\n",
    "\n",
    "# 3、获取存储的信息\n",
    "# 返回消息列表的方式1\n",
    "print(memory.load_memory_variables({}))\n",
    "print()\n",
    "# 返回消息列表的方式2\n",
    "print(memory.chat_memory.messages)"
   ],
   "id": "a0e6de0c6667637b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content='你好', additional_kwargs={}, response_metadata={}), AIMessage(content='你好！有什么我可以帮助你的吗？', additional_kwargs={}, response_metadata={}), HumanMessage(content='请问1.8何1.11谁大？', additional_kwargs={}, response_metadata={}), AIMessage(content='1.8比较大', additional_kwargs={}, response_metadata={})]}\n",
      "\n",
      "[HumanMessage(content='你好', additional_kwargs={}, response_metadata={}), AIMessage(content='你好！有什么我可以帮助你的吗？', additional_kwargs={}, response_metadata={}), HumanMessage(content='请问1.8何1.11谁大？', additional_kwargs={}, response_metadata={}), AIMessage(content='1.8比较大', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T15:00:34.209381Z",
     "start_time": "2025-10-04T15:00:31.911480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 举例3: 与LLM、提示词模板（PromptTemplate）集成\n",
    "import os\n",
    "import dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "# 1、加载环境变量\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY1\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "# 2、实例化ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 3、提供提示词模板\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\ttemplate=\"\"\"你可以与人类对话。\n",
    "\n",
    "当前对话历史: {history}\n",
    "\n",
    "人类问题: {question}\n",
    "\n",
    "回复:\n",
    "\t\"\"\"\n",
    ")\n",
    "\n",
    "# 4、实例化ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# 5、提供Chain\n",
    "chain = LLMChain(\n",
    "\tllm=llm,\n",
    "\tprompt=prompt_template,\n",
    "\tmemory=memory\n",
    ")\n",
    "\n",
    "# 6、进行对话\n",
    "response = chain.invoke({\"question\": \"你好,我的名字叫小明\"})\n",
    "# print(response)\n",
    "\n",
    "response = chain.invoke({\"question\": \"我叫什么名字？\"})\n",
    "print(response)"
   ],
   "id": "2b93a3d2904b078c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '我叫什么名字？', 'history': [HumanMessage(content='你好,我的名字叫小明', additional_kwargs={}, response_metadata={}), AIMessage(content='你好，小明！很高兴认识你。有什么我可以帮助你的吗？', additional_kwargs={}, response_metadata={}), HumanMessage(content='我叫什么名字？', additional_kwargs={}, response_metadata={}), AIMessage(content='你叫小明。', additional_kwargs={}, response_metadata={})], 'text': '你叫小明。'}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T15:03:18.175883Z",
     "start_time": "2025-10-04T15:03:16.457889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 举例4:基于举例3显示的设置memory的key的值\n",
    "import os\n",
    "import dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "# 1、加载环境变量\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY1\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "# 2、实例化ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 3、提供提示词模板\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\ttemplate=\"\"\"你可以与人类对话。\n",
    "\n",
    "当前对话历史: {chat_history}\n",
    "\n",
    "人类问题: {question}\n",
    "\n",
    "回复:\n",
    "\t\"\"\"\n",
    ")\n",
    "\n",
    "# 4、实例化ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# 5、提供Chain\n",
    "chain = LLMChain(\n",
    "\tllm=llm,\n",
    "\tprompt=prompt_template,\n",
    "\tmemory=memory\n",
    ")\n",
    "\n",
    "# 6、进行对话\n",
    "response = chain.invoke({\"question\": \"你好,我的名字叫小明\"})\n",
    "# print(response)\n",
    "\n",
    "response = chain.invoke({\"question\": \"我叫什么名字？\"})\n",
    "print(response)"
   ],
   "id": "cc67ab2aa3532d26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '我叫什么名字？', 'chat_history': 'Human: 你好,我的名字叫小明\\nAI: 你好，小明！很高兴认识你。有什么我可以帮助你的吗？', 'text': '你叫小明。'}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T15:09:09.322058Z",
     "start_time": "2025-10-04T15:09:07.258778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 举例5：结合大模型、提示词模板（ChatPromptTemplate）的使用\n",
    "# 1.导入相关包\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import MessagesPlaceholder, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 2.创建LLM\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini')\n",
    "# 3.创建Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "\t(\"system\", \"你是一个与人类对话的机器人。\"),\n",
    "\tMessagesPlaceholder(variable_name='history'),\n",
    "\t(\"human\", \"问题：{question}\")\n",
    "])\n",
    "# 4.创建Memory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "# 5.创建LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory)\n",
    "# 6.调用LLMChain\n",
    "res1 = llm_chain.invoke({\"question\": \"中国首都在哪里？\"})\n",
    "print(res1, end=\"\\n\\n\")\n",
    "res2 = llm_chain.invoke({\"question\": \"我刚刚问了什么\"})\n",
    "print(res2)"
   ],
   "id": "f1f41a5bff31300f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '中国首都在哪里？', 'history': [HumanMessage(content='中国首都在哪里？', additional_kwargs={}, response_metadata={}), AIMessage(content='中国的首都位于北京。', additional_kwargs={}, response_metadata={})], 'text': '中国的首都位于北京。'}\n",
      "\n",
      "{'question': '我刚刚问了什么', 'history': [HumanMessage(content='中国首都在哪里？', additional_kwargs={}, response_metadata={}), AIMessage(content='中国的首都位于北京。', additional_kwargs={}, response_metadata={}), HumanMessage(content='我刚刚问了什么', additional_kwargs={}, response_metadata={}), AIMessage(content='你刚刚问了中国首都在哪里。', additional_kwargs={}, response_metadata={})], 'text': '你刚刚问了中国首都在哪里。'}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| 特性 | 普通 PromptTemplate           | ChatPromptTemplate |\n",
    "|:----:|:---------------------------:|:-----------------:|\n",
    "| 历史存储时机 | 仅执行后存储                      | 执行前存储用户输入 + 执行后存储输出 |\n",
    "| 首次调用显示 | 仅显示问题（历史仍为空字符串）             | 显示完整问答对 |\n",
    "| 内部消息类型 | 拼接字符串                       | List[BaseMessage] |"
   ],
   "id": "7c55aef9e03463f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3、ConversationChain\n",
    "ConversationChain实际上是就是对 ConversationBufferMemory 和 LLMChain 进行了封装，并且提供一个默认格式的提示词模版（我们也可以不用），从而简化了初始化ConversationBufferMemory的步骤。\n"
   ],
   "id": "c45385996ac25eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T15:23:15.274919Z",
     "start_time": "2025-10-04T15:23:12.105187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 举例1\n",
    "import os\n",
    "import dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# 1、加载环境变量\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY1\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "# 2、实例化ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 3、提供提示词模板 - 修正：不要使用f-string\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\ttemplate=\"\"\"你可以与人类对话。\n",
    "\n",
    "当前对话历史: {history}\n",
    "\n",
    "人类问题: {input}\n",
    "\n",
    "回复:\"\"\"\n",
    ")\n",
    "\n",
    "# 4、创建ConversationChain\n",
    "chain = ConversationChain(\n",
    "\tllm=llm,\n",
    "\tprompt=prompt_template,\n",
    "\t# memory=memory # 如果不传memory参数，则默认使用ConversationBufferMemory\n",
    ")\n",
    "\n",
    "# 5、进行对话\n",
    "response = chain.invoke({\"input\": \"你好,我的名字叫小明\"})\n",
    "print(response)\n",
    "\n",
    "response = chain.invoke({\"input\": \"我叫什么名字？\"})\n",
    "print(response)"
   ],
   "id": "d9051717c4c8cca8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '你好,我的名字叫小明', 'history': '', 'response': '你好，小明！很高兴认识你。有什么我可以帮助你的吗？'}\n",
      "{'input': '我叫什么名字？', 'history': 'Human: 你好,我的名字叫小明\\nAI: 你好，小明！很高兴认识你。有什么我可以帮助你的吗？', 'response': '你叫小明。你还有其他想聊的吗？'}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T15:30:32.632750Z",
     "start_time": "2025-10-04T15:30:30.388723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 举例2：使用默认的提示词模板\n",
    "import os\n",
    "import dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# 1、加载环境变量\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY1\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "# 2、实例化ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 3、创建ConversationChain\n",
    "# 内部提供了默认的提示词模板，此模板中的变量是「input」、「history」\n",
    "chain = ConversationChain(\n",
    "\tllm=llm,\n",
    "\t# memory=memory # 如果不传memory参数，则默认使用ConversationBufferMemory\n",
    ")\n",
    "\n",
    "# 4、进行对话\n",
    "response = chain.invoke({\"input\": \"你好,我的名字叫小明\"})\n",
    "print(response)\n",
    "\n",
    "response = chain.invoke({\"input\": \"我叫什么名字？\"})\n",
    "print(response)"
   ],
   "id": "7018e4a86ac25e6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '你好,我的名字叫小明', 'history': '', 'response': '你好，小明！很高兴认识你！我叫AI助手。你今天过得怎么样？有什么特别的事情想分享吗？'}\n",
      "{'input': '我叫什么名字？', 'history': 'Human: 你好,我的名字叫小明\\nAI: 你好，小明！很高兴认识你！我叫AI助手。你今天过得怎么样？有什么特别的事情想分享吗？', 'response': '你叫小明！我记得你刚刚告诉我你的名字。你还有其他想聊的事情吗？'}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4、ConversationBufferWindowMemory\n",
    "在了解了ConversationBufferMemory记忆类后，我们知道了它能够无限的将历史对话信息填充到History中，从而给大模型提供上下文的背景。但这会导致内存量十分大 ，并且消耗的token是非常多的，此外，每个大模型都存在最大输入的Token限制。\n",
    "\n",
    "我们发现，过久远的对话数据往往并不能对当前轮次的问答提供有效的信息，LangChain 给出的解决方式是： ConversationBufferWindowMemory 模块。该记忆类会 保存一段时间内对话交互 的列表， 仅使用最近 K 个交互 。这样就使缓存区不会变得太大。"
   ],
   "id": "b222b8e1ba37319f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T15:39:38.907674Z",
     "start_time": "2025-10-04T15:39:38.898699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 举例1\n",
    "# 1.导入相关包\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# 2.实例化ConversationBufferWindowMemory对象，设定窗口阈值\n",
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "# 3.保存消息\n",
    "memory.save_context({\"input\": \"你好\"}, {\"output\": \"怎么了\"})\n",
    "memory.save_context({\"input\": \"你是谁\"}, {\"output\": \"我是AI助手\"})\n",
    "memory.save_context({\"input\": \"你的生日是哪天？\"}, {\"output\": \"我不清楚\"})\n",
    "# 4.读取内存中消息（返回消息内容的纯文本）\n",
    "print(memory.load_memory_variables({}))"
   ],
   "id": "78f076c653592047",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 你是谁\\nAI: 我是AI助手\\nHuman: 你的生日是哪天？\\nAI: 我不清楚'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chenxr\\AppData\\Local\\Temp\\ipykernel_20220\\2753770296.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=2)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T15:40:33.442540Z",
     "start_time": "2025-10-04T15:40:33.421570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 举例2\n",
    "# 1.导入相关包\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# 2.实例化ConversationBufferWindowMemory对象，设定窗口阈值\n",
    "memory = ConversationBufferWindowMemory(k=2, return_messages=True)\n",
    "# 3.保存消息\n",
    "memory.save_context({\"input\": \"你好\"}, {\"output\": \"怎么了\"})\n",
    "memory.save_context({\"input\": \"你是谁\"}, {\"output\": \"我是AI助手小智\"})\n",
    "memory.save_context({\"input\": \"初次对话，你能介绍一下你自己吗？\"}, {\"output\": \"当然可以了。我是一个无所不能的小智。\"})\n",
    "# 4.读取内存中消息（返回消息内容的纯文本）\n",
    "print(memory.load_memory_variables({}))"
   ],
   "id": "c5ed83ec405e61dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content='你是谁', additional_kwargs={}, response_metadata={}), AIMessage(content='我是AI助手小智', additional_kwargs={}, response_metadata={}), HumanMessage(content='初次对话，你能介绍一下你自己吗？', additional_kwargs={}, response_metadata={}), AIMessage(content='当然可以了。我是一个无所不能的小智。', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T15:43:54.713452Z",
     "start_time": "2025-10-04T15:43:50.152909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 举例3：结合llm、chain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "# 1.导入相关包\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "# 2.定义模版\n",
    "template = \"\"\"以下是人类与AI之间的友好对话描述。AI表现得很健谈，并提供了大量来自其上下文的具体细节。如果AI不知道问题的答案，它会表示不知道。\n",
    "当前对话：\n",
    "{history}\n",
    "Human: {question}\n",
    "AI:\"\"\"\n",
    "# 3.定义提示词模版\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "# 4.创建大模型\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# 5.实例化ConversationBufferWindowMemory对象，设定窗口阈值\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "# 6.定义LLMChain\n",
    "conversation_with_summary = LLMChain(\n",
    "\tllm=llm,\n",
    "\tprompt=prompt_template,\n",
    "\tmemory=memory,\n",
    "\tverbose=True,\n",
    ")\n",
    "# 7.执行链（第一次提问）\n",
    "respon1 = conversation_with_summary.invoke({\"question\": \"你好，我是孙小空\"})\n",
    "# print(respon1)\n",
    "# 8.执行链（第二次提问）\n",
    "respon2 = conversation_with_summary.invoke(\n",
    "\t{\"question\": \"我还有两个师弟，一个是猪小戒，一个是沙小僧\"})\n",
    "# print(respon2)\n",
    "# 9.执行链（第三次提问）\n",
    "respon3 = conversation_with_summary.invoke(\n",
    "\t{\"question\": \"我今年高考，竟然考上了1本\"})\n",
    "# print(respon3)\n",
    "# 10.执行链（第四次提问）\n",
    "respon4 = conversation_with_summary.invoke({\"question\": \"我叫什么？\"})\n",
    "print(respon4)"
   ],
   "id": "8ea889e389c4ced3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3m以下是人类与AI之间的友好对话描述。AI表现得很健谈，并提供了大量来自其上下文的具体细节。如果AI不知道问题的答案，它会表示不知道。\n",
      "当前对话：\n",
      "\n",
      "Human: 你好，我是孙小空\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3m以下是人类与AI之间的友好对话描述。AI表现得很健谈，并提供了大量来自其上下文的具体细节。如果AI不知道问题的答案，它会表示不知道。\n",
      "当前对话：\n",
      "Human: 你好，我是孙小空\n",
      "AI: 你好，孙小空！很高兴认识你！你今天过得怎么样？有什么想聊的话题吗？\n",
      "Human: 我还有两个师弟，一个是猪小戒，一个是沙小僧\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3m以下是人类与AI之间的友好对话描述。AI表现得很健谈，并提供了大量来自其上下文的具体细节。如果AI不知道问题的答案，它会表示不知道。\n",
      "当前对话：\n",
      "Human: 我还有两个师弟，一个是猪小戒，一个是沙小僧\n",
      "AI: 听起来你和你的师弟们都有很有趣的名字！猪小戒和沙小僧分别代表了《西游记》中的角色吗？你们平时一起做些什么活动呢？\n",
      "Human: 我今年高考，竟然考上了1本\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3m以下是人类与AI之间的友好对话描述。AI表现得很健谈，并提供了大量来自其上下文的具体细节。如果AI不知道问题的答案，它会表示不知道。\n",
      "当前对话：\n",
      "Human: 我今年高考，竟然考上了1本\n",
      "AI: 恭喜你考上了1本！这真是一个了不起的成就！你一定付出了很多努力。你打算在哪个专业学习呢？或者你有没有心仪的大学？\n",
      "Human: 我叫什么？\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "{'question': '我叫什么？', 'history': 'Human: 我今年高考，竟然考上了1本\\nAI: 恭喜你考上了1本！这真是一个了不起的成就！你一定付出了很多努力。你打算在哪个专业学习呢？或者你有没有心仪的大学？', 'text': '抱歉，我并不知道你的名字。你愿意告诉我吗？'}\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
